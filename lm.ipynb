{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language Model",
      "provenance": [],
      "authorship_tag": "ABX9TyO03wJZZcCAPVjjuClZPerz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iiacobac/language-model/blob/main/lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PELA2RdF8P7n"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from time import time\n",
        "\n",
        "MAX_VOCAB = 10000\n",
        "batch_size = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnZ22W2dggnz",
        "outputId": "6ad9cebc-67fd-407f-8cd6-ef38abb4540f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "class LSTMCell(nn.Module):\n",
        "\n",
        "\tdef __init__(self, input_size, hidden_size):\n",
        "\t\tsuper(LSTMCell, self).__init__()\n",
        "\t\tself.input_size = input_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.x2h = nn.Linear(input_size, 4 * hidden_size, bias=True)\n",
        "\t\tself.h2h = nn.Linear(hidden_size, 4 * hidden_size, bias=True)\n",
        "\t\tself.reset_parameters()\n",
        "\n",
        "\tdef reset_parameters(self):\n",
        "\t\tstd = 1.0 / math.sqrt(self.hidden_size)\n",
        "\t\tfor w in self.parameters():\n",
        "\t\t\tw.data.uniform_(-std, std)\n",
        "\n",
        "\tdef forward(self, x, hidden):\n",
        "\t\t#import pdb; pdb.set_trace()\n",
        "\t\thx, cx = hidden\n",
        "\t\tx = x.view(-1, x.size(1))\n",
        "\t\tgates = self.x2h(x) + self.h2h(hx)\n",
        "\t\tgates = gates.squeeze()\n",
        "\t\t\n",
        "\t\tingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
        "\n",
        "\t\tingate = torch.sigmoid(ingate)\n",
        "\t\tforgetgate = torch.sigmoid(forgetgate)\n",
        "\t\tcellgate = torch.tanh(cellgate)\n",
        "\t\toutgate = torch.sigmoid(outgate)\n",
        "\t\t\n",
        "\t\tcy = torch.mul(cx, forgetgate) + torch.mul(ingate, cellgate)\n",
        "\t\thy = torch.mul(outgate, torch.tanh(cy))\n",
        "\n",
        "\t\treturn (hy, cy)\n",
        "\n",
        "vocab = {}\n",
        "def load_data(file):\n",
        "\tvocab_idx = 0\n",
        "\twith open(file) as f:\n",
        "\t\ttext = f.read().replace(\"\\n\",\"<eos>\")\n",
        "\tarr = text.split()\n",
        "\tdata = np.zeros(len(arr), dtype='int32')\n",
        "\tfor i, word in enumerate(arr):\n",
        "\t\tif word not in vocab:\n",
        "\t\t\tvocab[word] = vocab_idx\n",
        "\t\t\tvocab_idx = vocab_idx + 1\n",
        "\t\tdata[i] = vocab[word]\n",
        "\treturn batcherize(np.array(data), batch_size)\n",
        "\n",
        "def batcherize(corpus, batch_size):\n",
        "\ts = len(corpus)\n",
        "\tx = np.zeros((batch_size, s // batch_size), dtype='int32')\n",
        "\tstart = 0\n",
        "\tfor i in range(batch_size):\n",
        "\t\tfinish = start + x.shape[1]\n",
        "\t\tx[i,:] = corpus[start:finish]\n",
        "\t\tstart = finish\n",
        "\treturn x\n",
        "\n",
        "class RNN(nn.Module):\n",
        "\tdef __init__(self, vocab_size, input_dim, hidden_dim, n_layers, dropouts, init_scale):\n",
        "\t\tsuper(RNN, self).__init__()\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.input_dim = hidden_dim\n",
        "\t\tself.hidden_dim = hidden_dim\n",
        "\t\tself.n_layers = n_layers\n",
        "\t\tself.dropouts = dropouts\n",
        "\t\tself.initrange = init_scale\n",
        "\t\t\n",
        "\t\tself.encoder = nn.Embedding(vocab_size, input_dim)\n",
        "\t\tself.drop = nn.Dropout(dropouts)\n",
        "\t\tself.rnn1 = LSTMCell(input_dim, hidden_dim)\n",
        "\t\tself.rnn2 = LSTMCell(input_dim, hidden_dim)\n",
        "\t\tself.decoder = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "\tdef forward(self, input, hidden):\n",
        "\t\t#import pdb;pdb.set_trace()\n",
        "\t\temb = self.drop(self.encoder(input))\n",
        "\t\tout1 = torch.zeros_like(emb)\n",
        "\t\thx, cx = hidden[0]\n",
        "\t\tfor step in range(emb.shape[1]):\n",
        "\t\t\thx, cx = self.rnn1(emb[:,step], (hx, cx))\n",
        "\t\t\tout1[:, step] = hx\n",
        "\t\tout1 = self.drop(out1)\n",
        "\t\tout2 = torch.zeros_like(emb)\n",
        "\t\thx2, cx2 = hidden[1]\n",
        "\t\tfor step in range(emb.shape[1]):\n",
        "\t\t\thx2, cx2 = self.rnn2(out1[:,step], (hx2, cx2))\n",
        "\t\t\tout2[:, step] = hx2\n",
        "\t\tout2 = self.drop(out2)\n",
        "\t\toutput = self.decoder(out2)\n",
        "\t\treturn output, ((hx, cx), (hx2, cx2))\n",
        "\n",
        "\tdef init_weights(self):\n",
        "\t\tinitrange = self.initrange\n",
        "\t\tself.encoder.weight.data.uniform_(-initrange,initrange)\n",
        "\t\tself.decoder.bias.data.zero_()\n",
        "\t\tself.decoder.weight.data.uniform_(-initrange,initrange)\n",
        "\n",
        "\tdef init_hidden(self, batch_size):\n",
        "\t\treturn (((Variable(torch.zeros(batch_size, self.hidden_dim)).cuda(),) * 2),) * self.n_layers\n",
        "\n",
        "def create_model(train_params):\n",
        "\tprint(train_params)\n",
        "\treturn RNN(MAX_VOCAB, train_params['hidden_size'], train_params['hidden_size'], 2, \n",
        "\t\ttrain_params['dropout'], train_params['init_scale'])\n",
        "\n",
        "def repackage_hidden(h):\n",
        "\tif isinstance(h, torch.Tensor):\n",
        "\t\treturn h.detach()\n",
        "\telse:\n",
        "\t\treturn tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def train_model(model, corpus, criterion, train_params, valid, test):\n",
        "\twin_size = int(train_params['win_size'])\n",
        "\tepochs = int(train_params['epochs'])\n",
        "\tepoch_size = train.shape[1] // win_size\n",
        "\tprint(\"epoch_size\", epoch_size)\n",
        "\tlr_decayed = train_params['learning_rate']\n",
        "\tfor cnt in range(epochs):\n",
        "\t\tif train_params['epoch_decay_start'] <= cnt:\n",
        "\t\t\tlr_decayed /= (train_params['decay'] * 1.0)\n",
        "\t\tmodel.train()\n",
        "\t\toptimizer = torch.optim.SGD(model.parameters(), lr=lr_decayed)\n",
        "\t\tpartial_loss = 0.0\n",
        "\t\thidden = model.init_hidden(batch_size)\n",
        "\t\tfor count_v, offset in enumerate(range(0, corpus.shape[1] - 1, win_size)):\n",
        "\t\t\tseq_len = int(min(win_size, corpus.shape[1] - offset - 1))\n",
        "\t\t\t\n",
        "\t\t\tX = torch.LongTensor(corpus[:,offset     : offset + seq_len    ]).cuda()\n",
        "\t\t\tY = torch.LongTensor(corpus[:,offset + 1 : offset + seq_len + 1]).cuda()\n",
        "\t\t\t\n",
        "\t\t\thidden = repackage_hidden(hidden)\n",
        "\t\t\t#import pdb; pdb.set_trace()\n",
        "\t\t\t# forward pass\n",
        "\t\t\toutput, hidden = model(X, hidden)\n",
        "\t\t\tloss = criterion(output.view(-1, MAX_VOCAB), Y.reshape(-1))\n",
        "\t\t\t\n",
        "\t\t\t# partial loss\n",
        "\t\t\tpartial_loss += loss.item()\n",
        "\t\t\t\n",
        "\t\t\t# zero loss\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\t\t\tloss.backward()\n",
        "\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), train_params['clip_norm'])\n",
        "\t\t\toptimizer.step()\n",
        "\t\t\t\n",
        "\t\t\tif count_v % (epoch_size // 10) == 50:\n",
        "\t\t\t\tloss = partial_loss / ((epoch_size // 10) * seq_len * batch_size * 1.0)\n",
        "\t\t\t\tprint(\"Iteration {:3.2f}, Offset {:5d}, loss={:6.5f}, perplexity={:6.5f}, lr={:3.4f}\".format(cnt + count_v / epoch_size, offset, loss, math.exp(loss), lr_decayed))\n",
        "\t\t\t\tpartial_loss = 0.0\n",
        "\t\tloss = test_model(model, train, criterion, default_params)\n",
        "\t\tloss = test_model(model, valid, criterion, default_params)\n",
        "\t\tloss = test_model(model, test, criterion, default_params)\n",
        "\n",
        "def test_model(model, corpus, criterion, params):\n",
        "\tmodel.eval()\n",
        "\twin_size = int(params['win_size'])\n",
        "\tavg = 0.0\n",
        "\tsum_v = 0.0\n",
        "\tcount_v = 0.0\n",
        "\toffset = 0\n",
        "\thidden = model.init_hidden(batch_size)\n",
        "\tlosses = 0.0\n",
        "\t\n",
        "\tfor count_v, offset in enumerate(range(0, corpus.shape[1] - 1, win_size)):\n",
        "\t\tseq_len = min(win_size, corpus.shape[1] - offset - 1)\n",
        "\t\tX = torch.LongTensor(corpus[:,offset     : offset + seq_len    ]).cuda()\n",
        "\t\tY = torch.LongTensor(corpus[:,offset + 1 : offset + seq_len + 1]).cuda()\n",
        "\t\thidden = repackage_hidden(hidden)\n",
        "\t\toutput, hidden = model(X, hidden)\n",
        "\t\tloss = criterion(output.view(-1, MAX_VOCAB), Y.reshape(-1))\n",
        "\t\tavg += loss.item()\n",
        "\t\tsum_v += np.prod(Y.size())\n",
        "\tavg = avg / float(sum_v)\n",
        "\tprint(\"avg_loss_v={}, perplexity={}\".format(avg, math.exp(avg)))\n",
        "\treturn avg\n",
        "\n",
        "default_params = {\n",
        "\t'clip_norm': 5.0,\n",
        "\t'learning_rate': 1.0,\n",
        "\t'hidden_size': 650,\n",
        "\t'epochs': 39,\n",
        "\t'win_size': 35,\n",
        "\t'epoch_decay_start': 6,\n",
        "\t'decay': 1.2,\n",
        "\t'dropout': 0.5,\n",
        "\t'init_scale': 0.05\n",
        "}\n",
        "\n",
        "train = load_data('ptb.train.txt')\n",
        "valid = load_data('ptb.valid.txt')\n",
        "test = load_data('ptb.test.txt')\n",
        "\n",
        "#import pdb; pdb.set_trace()\n",
        "train=torch.LongTensor(train.astype(np.int64))\n",
        "valid=torch.LongTensor(valid.astype(np.int64))\n",
        "test=torch.LongTensor(test.astype(np.int64))\n",
        "\n",
        "print(train.shape)\n",
        "print(len(vocab))\n",
        "\n",
        "model = create_model(default_params).cuda()\n",
        "model.init_weights()\n",
        "\n",
        "criterion=torch.nn.CrossEntropyLoss(size_average=False)\n",
        "train_model(model, train, criterion, default_params, valid, test)\n",
        "test_model(model, train, criterion, default_params)\n",
        "test_model(model, valid, criterion, default_params)\n",
        "test_model(model, test, criterion, default_params)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([20, 38202])\n",
            "9957\n",
            "{'clip_norm': 5.0, 'learning_rate': 1.0, 'hidden_size': 650, 'epochs': 39, 'win_size': 35, 'epoch_decay_start': 6, 'decay': 1.2, 'dropout': 0.5, 'init_scale': 0.05}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch_size 1091\n",
            "Iteration 0.05, Offset  1750, loss=3.62588, perplexity=37.55776, lr=1.0000\n",
            "Iteration 0.15, Offset  5565, loss=6.88582, perplexity=978.30606, lr=1.0000\n",
            "Iteration 0.25, Offset  9380, loss=6.47786, perplexity=650.57971, lr=1.0000\n",
            "Iteration 0.35, Offset 13195, loss=6.24207, perplexity=513.92010, lr=1.0000\n",
            "Iteration 0.45, Offset 17010, loss=6.08804, perplexity=440.55738, lr=1.0000\n",
            "Iteration 0.55, Offset 20825, loss=5.89887, perplexity=364.62455, lr=1.0000\n",
            "Iteration 0.65, Offset 24640, loss=5.77802, perplexity=323.11783, lr=1.0000\n",
            "Iteration 0.75, Offset 28455, loss=5.72054, perplexity=305.06868, lr=1.0000\n",
            "Iteration 0.85, Offset 32270, loss=5.64864, perplexity=283.90559, lr=1.0000\n",
            "Iteration 0.95, Offset 36085, loss=5.57650, perplexity=264.14646, lr=1.0000\n",
            "avg_loss_v=5.382010714284618, perplexity=217.45908421589544\n",
            "avg_loss_v=5.427621631296405, perplexity=227.60726716648273\n",
            "avg_loss_v=5.406756716237485, perplexity=222.90746189089387\n",
            "Iteration 1.05, Offset  1750, loss=2.56147, perplexity=12.95481, lr=1.0000\n",
            "Iteration 1.15, Offset  5565, loss=5.48079, perplexity=240.03544, lr=1.0000\n",
            "Iteration 1.25, Offset  9380, loss=5.43745, perplexity=229.85633, lr=1.0000\n",
            "Iteration 1.35, Offset 13195, loss=5.34084, perplexity=208.68719, lr=1.0000\n",
            "Iteration 1.45, Offset 17010, loss=5.39271, perplexity=219.79716, lr=1.0000\n",
            "Iteration 1.55, Offset 20825, loss=5.29101, perplexity=198.54448, lr=1.0000\n",
            "Iteration 1.65, Offset 24640, loss=5.25952, perplexity=192.38910, lr=1.0000\n",
            "Iteration 1.75, Offset 28455, loss=5.24544, perplexity=189.69867, lr=1.0000\n",
            "Iteration 1.85, Offset 32270, loss=5.22823, perplexity=186.46293, lr=1.0000\n",
            "Iteration 1.95, Offset 36085, loss=5.19196, perplexity=179.82025, lr=1.0000\n",
            "avg_loss_v=4.9869238120517645, perplexity=146.48511395296123\n",
            "avg_loss_v=5.124208887036589, perplexity=168.04114952110976\n",
            "avg_loss_v=5.09995409326646, perplexity=164.01437776273812\n",
            "Iteration 2.05, Offset  1750, loss=2.39452, perplexity=10.96292, lr=1.0000\n",
            "Iteration 2.15, Offset  5565, loss=5.15339, perplexity=173.01644, lr=1.0000\n",
            "Iteration 2.25, Offset  9380, loss=5.13053, perplexity=169.10724, lr=1.0000\n",
            "Iteration 2.35, Offset 13195, loss=5.04800, perplexity=155.71058, lr=1.0000\n",
            "Iteration 2.45, Offset 17010, loss=5.12776, perplexity=168.63977, lr=1.0000\n",
            "Iteration 2.55, Offset 20825, loss=5.03195, perplexity=153.23079, lr=1.0000\n",
            "Iteration 2.65, Offset 24640, loss=5.01836, perplexity=151.16394, lr=1.0000\n",
            "Iteration 2.75, Offset 28455, loss=5.01127, perplexity=150.09520, lr=1.0000\n",
            "Iteration 2.85, Offset 32270, loss=5.01885, perplexity=151.23671, lr=1.0000\n",
            "Iteration 2.95, Offset 36085, loss=4.97973, perplexity=145.43517, lr=1.0000\n",
            "avg_loss_v=4.7434268182005725, perplexity=114.82701955856417\n",
            "avg_loss_v=4.968043271823404, perplexity=143.7453414464668\n",
            "avg_loss_v=4.937272972921724, perplexity=139.38961154569154\n",
            "Iteration 3.05, Offset  1750, loss=2.29924, perplexity=9.96663, lr=1.0000\n",
            "Iteration 3.15, Offset  5565, loss=4.96119, perplexity=142.76412, lr=1.0000\n",
            "Iteration 3.25, Offset  9380, loss=4.94243, perplexity=140.11082, lr=1.0000\n",
            "Iteration 3.35, Offset 13195, loss=4.86903, perplexity=130.19448, lr=1.0000\n",
            "Iteration 3.45, Offset 17010, loss=4.95312, perplexity=141.61666, lr=1.0000\n",
            "Iteration 3.55, Offset 20825, loss=4.86615, perplexity=129.81985, lr=1.0000\n",
            "Iteration 3.65, Offset 24640, loss=4.86302, perplexity=129.41458, lr=1.0000\n",
            "Iteration 3.75, Offset 28455, loss=4.83841, perplexity=126.26790, lr=1.0000\n",
            "Iteration 3.85, Offset 32270, loss=4.86560, perplexity=129.74852, lr=1.0000\n",
            "Iteration 3.95, Offset 36085, loss=4.82863, perplexity=125.03992, lr=1.0000\n",
            "avg_loss_v=4.565700130872566, perplexity=96.1298739539307\n",
            "avg_loss_v=4.869240535925422, perplexity=130.2219804160166\n",
            "avg_loss_v=4.835990647991884, perplexity=125.96330671497455\n",
            "Iteration 4.05, Offset  1750, loss=2.23221, perplexity=9.32042, lr=1.0000\n",
            "Iteration 4.15, Offset  5565, loss=4.81677, perplexity=123.56533, lr=1.0000\n",
            "Iteration 4.25, Offset  9380, loss=4.79850, perplexity=121.32827, lr=1.0000\n",
            "Iteration 4.35, Offset 13195, loss=4.73013, perplexity=113.31075, lr=1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdbt_qDpgwqL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I772HyRhFPo",
        "outputId": "1efef139-fd2e-4680-8a5f-70a7756167cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGum3SpFhHBv",
        "outputId": "479c825d-2c02-46f1-ffdd-f512ffa170db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ptb.test.txt  ptb.valid.txt  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCzSOSuJhIh4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}